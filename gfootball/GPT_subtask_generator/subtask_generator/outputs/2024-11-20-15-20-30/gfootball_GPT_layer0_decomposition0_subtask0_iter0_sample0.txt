[DEBUG 15:22:26] git.cmd Popen(['git', 'version'], cwd=/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/gfootball/GPT_subtask_generator/subtask_generator, stdin=None, shell=False, universal_newlines=False)
[DEBUG 15:22:26] git.cmd Popen(['git', 'version'], cwd=/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/gfootball/GPT_subtask_generator/subtask_generator, stdin=None, shell=False, universal_newlines=False)
[INFO 15:22:26] root Saving to FileStorageObserver in results/sacred.
[DEBUG 15:22:26] pymarl Using capture mode "fd"
[INFO 15:22:26] pymarl Running command 'my_main'
[INFO 15:22:26] pymarl Started run with ID "1"
[DEBUG 15:22:26] pymarl Starting Heartbeat
[DEBUG 15:22:26] my_main Started
[WARNING 15:22:26] my_main CUDA flag use_cuda was switched OFF automatically because no CUDA devices are available!
AAAAAAAAAA namespace(runner='parallel', mac='basic_mac', env='gfootball', common_reward=True, reward_scalarisation='sum', env_args={'map_name': 'scenario_layer0_decomposition0_subtask0', 'num_agents': 2, 'representation': 'simple115', 'rewards': 'scoring, reward_layer0_decomposition0_subtask0_iter0_sample0', 'time_limit': 150, 'seed': 238888787}, batch_size_run=10, test_nepisode=30, test_interval=10000, test_greedy=True, log_interval=10000, runner_log_interval=10000, learner_log_interval=10000, t_max=1000000, use_cuda=False, buffer_cpu_only=True, use_tensorboard=True, use_wandb=False, wandb_team=None, wandb_project=None, wandb_mode='offline', wandb_save_model=False, save_model=False, save_model_interval=50000, checkpoint_path='', evaluate=False, render=False, load_step=0, save_replay=False, local_results_path='results', gamma=0.99, batch_size=10, buffer_size=10, lr=0.0005, optim_alpha=0.99, optim_eps=1e-05, grad_norm_clip=10, add_value_last_step=True, agent='rnn', hidden_dim=128, obs_agent_id=True, obs_last_action=False, repeat_id=1, label='default_label', hypergroup=None, action_selector='soft_policies', agent_output_type='pi_logits', critic_type='ac_critic', decomposition_id=0, doe_classifier_cfg={'doe_type': 'mlp', 'load_doe_buffer_path': 'GRF_SUBTASK/doe_epymarl-main/results/buffer/grf', 'load_doe_name': 'load_mlp_classifier.pt', 'load_mode': 'train', 'mlp': {'batch_size': 512, 'hidden_sizes': [128], 'learning_rate': '1e-2', 'test_fraction': 0.1}, 'role_ids': {'task': [0, 1]}, 'save_classifier': True, 'save_doe_name': 'save_mlp_classifier.pt'}, entropy_coef=0.001, group_id=0, iter_id=0, layer_id=0, learner='actor_critic_learner', mask_before_softmax=True, name='ia2c', obs_individual_obs=False, q_nstep=5, sample_id=0, save_buffer=True, save_doe_cls=True, standardise_returns=False, standardise_rewards=True, target_update_interval_or_tau=0.01, time_stamp='2024-11-20-15-20-30', use_doe=False, use_rnn=True, seed=238888787, device='cpu')
[INFO 15:22:26] my_main Experiment Parameters:
[INFO 15:22:26] my_main 

{   'action_selector': 'soft_policies',
    'add_value_last_step': True,
    'agent': 'rnn',
    'agent_output_type': 'pi_logits',
    'batch_size': 10,
    'batch_size_run': 10,
    'buffer_cpu_only': True,
    'buffer_size': 10,
    'checkpoint_path': '',
    'common_reward': True,
    'critic_type': 'ac_critic',
    'decomposition_id': 0,
    'doe_classifier_cfg': {   'doe_type': 'mlp',
                              'load_doe_buffer_path': 'GRF_SUBTASK/doe_epymarl-main/results/buffer/grf',
                              'load_doe_name': 'load_mlp_classifier.pt',
                              'load_mode': 'train',
                              'mlp': {   'batch_size': 512,
                                         'hidden_sizes': [   128],
                                         'learning_rate': '1e-2',
                                         'test_fraction': 0.1},
                              'role_ids': {   'task': [   0,
                                                          1]},
                              'save_classifier': True,
                              'save_doe_name': 'save_mlp_classifier.pt'},
    'entropy_coef': 0.001,
    'env': 'gfootball',
    'env_args': {   'map_name': 'scenario_layer0_decomposition0_subtask0',
                    'num_agents': 2,
                    'representation': 'simple115',
                    'rewards': 'scoring, '
                               'reward_layer0_decomposition0_subtask0_iter0_sample0',
                    'seed': 238888787,
                    'time_limit': 150},
    'evaluate': False,
    'gamma': 0.99,
    'grad_norm_clip': 10,
    'group_id': 0,
    'hidden_dim': 128,
    'hypergroup': None,
    'iter_id': 0,
    'label': 'default_label',
    'layer_id': 0,
    'learner': 'actor_critic_learner',
    'learner_log_interval': 10000,
    'load_step': 0,
    'local_results_path': 'results',
    'log_interval': 10000,
    'lr': 0.0005,
    'mac': 'basic_mac',
    'mask_before_softmax': True,
    'name': 'ia2c',
    'obs_agent_id': True,
    'obs_individual_obs': False,
    'obs_last_action': False,
    'optim_alpha': 0.99,
    'optim_eps': 1e-05,
    'q_nstep': 5,
    'render': False,
    'repeat_id': 1,
    'reward_scalarisation': 'sum',
    'runner': 'parallel',
    'runner_log_interval': 10000,
    'sample_id': 0,
    'save_buffer': True,
    'save_doe_cls': True,
    'save_model': False,
    'save_model_interval': 50000,
    'save_replay': False,
    'seed': 238888787,
    'standardise_returns': False,
    'standardise_rewards': True,
    't_max': 1000000,
    'target_update_interval_or_tau': 0.01,
    'test_greedy': True,
    'test_interval': 10000,
    'test_nepisode': 30,
    'time_stamp': '2024-11-20-15-20-30',
    'use_cuda': False,
    'use_doe': False,
    'use_rnn': True,
    'use_tensorboard': True,
    'use_wandb': False,
    'wandb_mode': 'offline',
    'wandb_project': None,
    'wandb_save_model': False,
    'wandb_team': None}

[INFO 15:22:26] my_main *******************
[INFO 15:22:26] my_main Tensorboard logging dir:
[INFO 15:22:26] my_main /home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/doe_epymarl-main/results/tb_logs/2024-11-20-15-20-30/layer0_decomposition0_subtask0_iter0_sample0
[INFO 15:22:26] my_main *******************
[INFO 15:22:27] my_main Beginning training for 1000000 timesteps
[DEBUG 15:22:27] absl Dump "episode_done": count limit reached / disabled
[DEBUG 15:22:27] absl Dump "episode_done": count limit reached / disabled
[DEBUG 15:22:27] absl Dump "episode_done": count limit reached / disabled
[DEBUG 15:22:27] absl Dump "episode_done": count limit reached / disabled
[DEBUG 15:22:27] absl Dump "episode_done": count limit reached / disabled
[DEBUG 15:22:27] absl Dump "episode_done": count limit reached / disabled
[DEBUG 15:22:27] absl Dump "episode_done": count limit reached / disabled
[DEBUG 15:22:27] absl Dump "episode_done": count limit reached / disabled
[DEBUG 15:22:27] absl Dump "episode_done": count limit reached / disabled
[DEBUG 15:22:27] absl Dump "episode_done": count limit reached / disabled
Process Process-6:
Process Process-9:
Traceback (most recent call last):
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/doe_epymarl-main/src/runners/parallel_runner.py", line 287, in env_worker
    reward, terminated, env_info = env.step(actions)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/doe_epymarl-main/src/envs/gfootball/FootballEnv.py", line 84, in step
    obs, rewards, done, infos = self.env.step(actions.tolist())
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/site-packages/gym/core.py", line 280, in step
    return self.env.step(action)
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/site-packages/gym/core.py", line 314, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/gfootball/rewards/reward_layer0_decomposition0_subtask0_iter0_sample0.py", line 64, in step
    reward, components = self.reward(reward)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/gfootball/rewards/reward_layer0_decomposition0_subtask0_iter0_sample0.py", line 53, in reward
    distance_to_goal = np.linalg.norm(np.array(goal_position) - np.array(obs['ball']))
ValueError: operands could not be broadcast together with shapes (2,) (3,) 
Traceback (most recent call last):
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/doe_epymarl-main/src/runners/parallel_runner.py", line 287, in env_worker
    reward, terminated, env_info = env.step(actions)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/doe_epymarl-main/src/envs/gfootball/FootballEnv.py", line 84, in step
    obs, rewards, done, infos = self.env.step(actions.tolist())
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/site-packages/gym/core.py", line 280, in step
    return self.env.step(action)
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/site-packages/gym/core.py", line 314, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/gfootball/rewards/reward_layer0_decomposition0_subtask0_iter0_sample0.py", line 64, in step
    reward, components = self.reward(reward)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/gfootball/rewards/reward_layer0_decomposition0_subtask0_iter0_sample0.py", line 53, in reward
    distance_to_goal = np.linalg.norm(np.array(goal_position) - np.array(obs['ball']))
ValueError: operands could not be broadcast together with shapes (2,) (3,) 
Process Process-2:
Traceback (most recent call last):
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/doe_epymarl-main/src/runners/parallel_runner.py", line 287, in env_worker
    reward, terminated, env_info = env.step(actions)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/doe_epymarl-main/src/envs/gfootball/FootballEnv.py", line 84, in step
    obs, rewards, done, infos = self.env.step(actions.tolist())
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/site-packages/gym/core.py", line 280, in step
    return self.env.step(action)
  File "/home/zihao/anaconda3/envs/grf2/lib/python3.10/site-packages/gym/core.py", line 314, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/gfootball/rewards/reward_layer0_decomposition0_subtask0_iter0_sample0.py", line 64, in step
    reward, components = self.reward(reward)
  File "/home/zihao/zihao/PycharmProjects/GRF_SUBTASK-1118/gfootball/rewards/reward_layer0_decomposition0_subtask0_iter0_sample0.py", line 53, in reward
    distance_to_goal = np.linalg.norm(np.array(goal_position) - np.array(obs['ball']))
ValueError: operands could not be broadcast together with shapes (2,) (3,) 
