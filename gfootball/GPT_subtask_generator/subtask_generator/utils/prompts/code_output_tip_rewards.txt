The reply should be the code of a gym-style reward wrapper: "class CheckpointRewardWrapper(gym.RewardWrapper)"
Like the given example, The class must have five functions:
def reset(self), def get_state(self, to_pickle), def set_state(self, state), def reward(self, reward) and def step(self, action).
Make sure you don't change the step(self, action) function, which adds component values and final reward values to info.
In the function def reward(self, reward), you can use the observations from the environment to define your own reward function.
The observation is a list of length {number_of_agents}. Each element o of the observation list is the observation dictionary dict corresponding to the agent. An example of o is {example_of_o}
The output of the reward(self, reward) function should consist of two items:
    (1) a list of length {number_of_agents}. Each element of the list is a scalar value of the corresponding agent.
    (2) a dictionary of each individual reward component.
The signature of the reward(self, reward) should be: {reward_signature}
Some important rules that you must follow:
    (1) The code output should be formatted as a python code string: "```python ... ```".
    (2) Write your own CheckpointRewardWrapper with a new reward function, and try to use other keys in the observation dictionary.
    (3) The example just helps you to understand the output structure, but you should not just copy the contents.
    (4) Ensure that in the reward(self, reward) function, all return values directed by the if statements adhere to the output requirements, which include a reward list and a components dictionary.
    (5) Ensure that the "base_score_reward" is included in the reward components. This component represents the reward before it is wrapped, and you can use it in any way you prefer.
    (6) It's a good idea to introduce a coefficient parameter to each component, making it easier to adjust the reward function later